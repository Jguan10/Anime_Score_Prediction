{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing pulling data from MAL and formatting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import pickle\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import requests\n",
    "from scipy.sparse import load_npz\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.saving.load_model('Models/prediction_model.keras')\n",
    "\n",
    "with open('Models/tfidf_vectorizer.pkl', 'rb') as file:\n",
    "    vectorizer = pickle.load(file)\n",
    "\n",
    "with open('Models/rank_encoder.pkl', 'rb') as file:\n",
    "    rank_encoder = pickle.load(file)\n",
    "\n",
    "with open('Models/popularity_encoder.pkl', 'rb') as file:\n",
    "    popularity_encoder = pickle.load(file)\n",
    "\n",
    "with open('Models/score_encoder.pkl', 'rb') as file:\n",
    "    score_encoder = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape individual web_ids from MAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_id = '1'\n",
    "\n",
    "def scrape_all(anime_id):\n",
    "    # Construct the URL using the anime ID\n",
    "    url = f\"https://myanimelist.net/anime/{anime_id}\"\n",
    "    \n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve data. HTTP status code: {response.status_code}\")\n",
    "        return None\n",
    "    \n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Extract the entire page content as plain text\n",
    "    page_text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "    \n",
    "    return page_text\n",
    "\n",
    "\n",
    "anime_data = scrape_all(anime_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_list = anime_data.split('\\n')\n",
    "\n",
    "anime_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Column\n",
    "title = anime_list[0].split(' - ')[0]\n",
    "anime_list.pop(0)\n",
    "\n",
    "anime_dict['title'] = title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Type, Episodes, English Title, Source\n",
    "def parse_list(param, anime_list):\n",
    "    key = param.replace(':', '').strip().lower()\n",
    "    for i in range(len(anime_list)):\n",
    "        if anime_list[i].startswith(param):\n",
    "            anime_dict[key] = anime_list[i + 1].strip() \n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_list('Type:', anime_list)\n",
    "parse_list('Episodes:', anime_list)\n",
    "parse_list('English:', anime_list)\n",
    "parse_list('Source:', anime_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "synopsis_list = []\n",
    "\n",
    "found_index = -1\n",
    "\n",
    "for i in range(1, len(anime_list)):\n",
    "    if anime_list[i] == \"Synopsis\" and \"Edit\" in anime_list[i - 1]:\n",
    "        found_index = i\n",
    "        break\n",
    "\n",
    "if found_index != -1:\n",
    "    for i in range(found_index + 1, len(anime_list)):\n",
    "        if (anime_list[i].startswith(\"[Written by\") or \n",
    "            anime_list[i].startswith(\"Related Entries\") or \n",
    "            anime_list[i].startswith(\"Background\") or\n",
    "            anime_list[i].startswith(\"Edit\")):\n",
    "            break\n",
    "        synopsis_list.append(anime_list[i])\n",
    "\n",
    "synopsis_list \n",
    "combined_synopsis = \" \".join((synopsis_list))\n",
    "anime_dict['synopsis'] = combined_synopsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres_types = ['Action', 'Adventure', 'Avant Garde', 'Award Winning', 'Boys Love', \n",
    "                  'Comedy', 'Drama', 'Ecchi', 'Erotica', 'Fantasy', 'Girls Love', \n",
    "                  'Gourmet', 'Hentai', 'Horror', 'Mystery', 'Romance', 'Sci-Fi', \n",
    "                  'Slice of Life', 'Sports', 'Supernatural', 'Suspense']\n",
    "\n",
    "found_genres = set()\n",
    "\n",
    "for entry in anime_list:\n",
    "    if entry in genres_types:\n",
    "        found_genres.add(entry)\n",
    "\n",
    "genres = \", \".join(found_genres)\n",
    "anime_dict['genres'] = genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_studios = [\n",
    "    \"Toei Animation\", \"Sunrise\", \"J.C.Staff\", \"Madhouse\", \n",
    "    \"TMS Entertainment\", \"Production I.G\", \"Studio Deen\", \n",
    "    \"Pierrot\", \"OLM\", \"Shin-Ei Animation\", \"A-1 Pictures\", \n",
    "    \"Nippon Animation\", \"AIC\", \"DLE\", \"Tatsunoko Production\", \"Trigger\"\n",
    "]\n",
    "\n",
    "found_studios = set()\n",
    "\n",
    "for entry in anime_list:\n",
    "    if entry in top_studios:\n",
    "        found_studios.add(entry)\n",
    "\n",
    "studios = \", \".join(found_studios)\n",
    "anime_dict['studios'] = studios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_producers = [\"Aniplex\", \"TV Tokyo\", \"Lantis\", \"Movic\", \n",
    "                 \"AT-X\", \"Bandai Visual\", \"Pony Canyon\", \"Kadokawa\", \n",
    "                 \"Dentsu\", \"Fuji TV\", \"NHK\", \"Sotsu\", \"KlockWorx\", \"Kodansha\", \"Shueisha\"]\n",
    "\n",
    "found_producers = set()\n",
    "for entry in anime_list:\n",
    "    if entry in top_producers:\n",
    "        found_producers.add(entry)\n",
    "\n",
    "producers = \", \".join(found_producers)\n",
    "anime_dict['producers'] = producers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "if anime_dict.get('episodes') == 'Unknown':\n",
    "    anime_dict['episodes'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Cowboy Bebop',\n",
       " 'type': 'TV',\n",
       " 'episodes': '26',\n",
       " 'english': 'Cowboy Bebop',\n",
       " 'source': 'Original',\n",
       " 'synopsis': \"Crime is timeless. By the year 2071, humanity has expanded across the galaxy, filling the surface of other planets with settlements like those on Earth. These new societies are plagued by murder, drug use, and theft, and intergalactic outlaws are hunted by a growing number of tough bounty hunters. Spike Spiegel and Jet Black pursue criminals throughout space to make a humble living. Beneath his goofy and aloof demeanor, Spike is haunted by the weight of his violent past. Meanwhile, Jet manages his own troubled memories while taking care of Spike and the Bebop, their ship. The duo is joined by the beautiful con artist Faye Valentine, odd child Edward Wong Hau Pepelu Tivrusky IV, and Ein, a bioengineered Welsh Corgi. While developing bonds and working to catch a colorful cast of criminals, the Bebop crew's lives are disrupted by a menace from Spike's past. As a rival's maniacal plot continues to unravel, Spike must choose between life with his newfound family or revenge for his old wounds.\",\n",
       " 'genres': 'Award Winning, Sci-Fi, Action',\n",
       " 'studios': 'Sunrise',\n",
       " 'producers': 'Bandai Visual'}"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anime_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now format the set so it can be used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>type</th>\n",
       "      <th>episodes</th>\n",
       "      <th>english</th>\n",
       "      <th>source</th>\n",
       "      <th>synopsis</th>\n",
       "      <th>genres</th>\n",
       "      <th>studios</th>\n",
       "      <th>producers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cowboy Bebop</td>\n",
       "      <td>TV</td>\n",
       "      <td>26</td>\n",
       "      <td>Cowboy Bebop</td>\n",
       "      <td>Original</td>\n",
       "      <td>Crime is timeless. By the year 2071, humanity ...</td>\n",
       "      <td>Award Winning, Sci-Fi, Action</td>\n",
       "      <td>Sunrise</td>\n",
       "      <td>Bandai Visual</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          title type episodes       english    source  \\\n",
       "0  Cowboy Bebop   TV       26  Cowboy Bebop  Original   \n",
       "\n",
       "                                            synopsis  \\\n",
       "0  Crime is timeless. By the year 2071, humanity ...   \n",
       "\n",
       "                          genres  studios      producers  \n",
       "0  Award Winning, Sci-Fi, Action  Sunrise  Bandai Visual  "
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame([anime_dict])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatize and vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "custom_words = {'and', 'the', 'is', 'a', 'to', 'it', 's', 'like', 'year'}\n",
    "pattern = r'\\b(?:' + '|'.join(re.escape(word) for word in custom_words) + r')\\b'\n",
    "\n",
    "def is_capitalized(word):\n",
    "    return word[0].isupper() and word.isalpha()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    if pd.isnull(text): \n",
    "        return text\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    lemmatized_words = [\n",
    "        lemmatizer.lemmatize(word.lower()) for word in words\n",
    "        if word.lower() not in stop_words and word.lower() not in custom_words and not is_capitalized(word)\n",
    "    ]\n",
    "    lemmatized_text = ' '.join(lemmatized_words)\n",
    "\n",
    "    ## Futher clean anything lemmatization missed, remove spaces and characters\n",
    "    cleaned_text = re.sub(r'[^\\w\\s]', '', lemmatized_text)\n",
    "    cleaned_text = re.sub(pattern, '', cleaned_text, flags=re.IGNORECASE)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['synopsis'] = X_test['synopsis'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "synopsis_tfidf = vectorizer.transform(X_test['synopsis'])\n",
    "tfidf_df = pd.DataFrame(synopsis_tfidf.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "tfidf_df.columns = ['tfidf_' + col for col in tfidf_df.columns]\n",
    "X_test = pd.concat([X_test, tfidf_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hot encode other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "for producer in top_producers:\n",
    "    X_test[f'producer_{producer.replace(\" \", \"_\").lower()}'] = False\n",
    "\n",
    "for index, row in X_test.iterrows():\n",
    "    producers_in_row = row['producers']\n",
    "    \n",
    "    for producer in top_producers:\n",
    "        column_name = f'producer_{producer.replace(\" \", \"_\").lower()}'\n",
    "        if producer in producers_in_row:\n",
    "            X_test.at[index, column_name] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "for studio in top_studios:\n",
    "    X_test[f'studio_{studio.replace(\" \", \"_\").lower()}'] = False\n",
    "\n",
    "for index, row in X_test.iterrows():\n",
    "    studios_in_row = [studios.strip() for studios in row['studios'].split(',')]\n",
    "    \n",
    "    for studio in top_studios:\n",
    "        column_name = f'studio_{studio.replace(\" \", \"_\").lower()}'\n",
    "        if studio in studios_in_row:\n",
    "            X_test.at[index, column_name] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "for genre in genres_types:\n",
    "    X_test[f'Genres_{genre.replace(\" \", \"_\")}'] = False\n",
    "\n",
    "for index, row in X_test.iterrows():\n",
    "    genres_in_row = [genre.strip() for genre in row['genres'].split(',')]\n",
    "    \n",
    "    for genre in genres_types:\n",
    "        column_name = f'Genres_{studio}'\n",
    "        if genre in genres_in_row:\n",
    "            X_test.at[index, column_name] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['source'] = X_test['source'].replace('Unknown', np.nan)\n",
    "X_test['source'] = X_test['source'].replace('Mixed media', np.nan)\n",
    "X_test['source'] = X_test['source'].replace('Radio', np.nan)\n",
    "X_test['source'] = X_test['source'].replace('Card game', 'Game')\n",
    "X_test['source'] = X_test['source'].replace('Picture book', 'Book')\n",
    "X_test['source'] = X_test['source'].replace('Other', np.nan)\n",
    "X_test['source'] = X_test['source'].replace('Web manga', 'Manga')\n",
    "X_test['source'] = X_test['source'].replace('4-koma manga', 'Manga')\n",
    "X_test['source'] = X_test['source'].replace('Music', np.nan)\n",
    "X_test['source'] = X_test['source'].replace('Web novel', 'Book')\n",
    "X_test['source'] = X_test['source'].replace('Novel', 'Book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_columns = ['Source_Book', 'Source_Game', 'Source_Light novel', 'Source_Manga', 'Source_Original', 'Source_Visual novel']\n",
    "type_columns = ['Types_Movie','Types_Music','Types_ONA','Types_OVA','Types_Special','Types_TV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in source_columns:\n",
    "    source_type = col.split('_')[-1]  \n",
    "    X_test[col] = X_test['source'].apply(lambda x: True if isinstance(x, str) and source_type in x else False)\n",
    "\n",
    "for col in type_columns:\n",
    "    type_value = col.split('_')[-1]  \n",
    "    X_test[col] = X_test['type'].apply(lambda x: True if isinstance(x, str) and type_value in x else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.drop(columns = ['synopsis', 'source', 'genres', 'studios', 'producers'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.MinMaxScaler()\n",
    "X_test[[\"episodes\"]] = scaler.fit_transform(X_test[[\"episodes\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('Data/training_set.csv')\n",
    "X_test = X_test.reindex(columns=df1.columns, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['anime_id', 'title', 'episodes', 'score', 'Popularity_category',\n",
       "       'Rank_category', 'Genres_Action', 'Genres_Adventure',\n",
       "       'Genres_Avant Garde', 'Genres_Award Winning',\n",
       "       ...\n",
       "       'producer_pony_canyon', 'producer_kadokawa', 'producer_dentsu',\n",
       "       'producer_fuji_tv', 'producer_nhk', 'producer_sotsu',\n",
       "       'producer_klockworx', 'producer_kodansha', 'producer_shueisha', 'year'],\n",
       "      dtype='object', length=239)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.drop(columns = ['title', 'anime_id', 'Popularity_category', 'Rank_category', 'score', 'popularity', 'rank', 'studios', 'year'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_score = y_pred[0].argmax(axis=1)\n",
    "y_pred_score = score_encoder.inverse_transform(y_pred_score)\n",
    "\n",
    "y_pred_pop = y_pred[1].argmax(axis=1)\n",
    "y_pred_pop = popularity_encoder.inverse_transform(y_pred_pop)\n",
    "\n",
    "y_pred_rank = y_pred[2].argmax(axis=1)\n",
    "y_pred_rank = rank_encoder.inverse_transform(y_pred_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['7'], dtype=object),\n",
       " array(['Top 5,000'], dtype=object),\n",
       " array(['Top 5,000'], dtype=object))"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_score, y_pred_pop, y_pred_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from playwright.sync_api import sync_playwright\n",
    "import asyncio\n",
    "\n",
    "async def fetch_images(url):\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)  # Run in headless mode\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url)\n",
    "\n",
    "        # Extract all image URLs\n",
    "        img_urls = await page.evaluate('''() => {\n",
    "            return Array.from(document.querySelectorAll('img')).map(img => img.src);\n",
    "        }''')\n",
    "\n",
    "        # Filter URLs that start with the specified string\n",
    "        filtered_img_urls = [img_url for img_url in img_urls if img_url.startswith(\"https://cdn.myanimelist.net/images/anime\")]\n",
    "\n",
    "        await browser.close()\n",
    "        return filtered_img_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_id = '54857'\n",
    "url = f\"https://myanimelist.net/anime/{anime_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "This event loop is already running",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m loop \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mget_event_loop()\n\u001b[0;32m      2\u001b[0m future \u001b[38;5;241m=\u001b[39m loop\u001b[38;5;241m.\u001b[39mcreate_task(fetch_images(url))\n\u001b[1;32m----> 3\u001b[0m image_urls \u001b[38;5;241m=\u001b[39m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, img_url \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(image_urls):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(img_url)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\asyncio\\base_events.py:625\u001b[0m, in \u001b[0;36mBaseEventLoop.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m    614\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run until the Future is done.\u001b[39;00m\n\u001b[0;32m    615\u001b[0m \n\u001b[0;32m    616\u001b[0m \u001b[38;5;124;03mIf the argument is a coroutine, it is wrapped in a Task.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;124;03mReturn the Future's result, or raise its exception.\u001b[39;00m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m--> 625\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_running\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    627\u001b[0m new_task \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m futures\u001b[38;5;241m.\u001b[39misfuture(future)\n\u001b[0;32m    628\u001b[0m future \u001b[38;5;241m=\u001b[39m tasks\u001b[38;5;241m.\u001b[39mensure_future(future, loop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\asyncio\\base_events.py:584\u001b[0m, in \u001b[0;36mBaseEventLoop._check_running\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_running\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_running():\n\u001b[1;32m--> 584\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis event loop is already running\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    586\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    587\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot run the event loop while another loop is running\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: This event loop is already running"
     ]
    }
   ],
   "source": [
    "image_urls = await fetch_images(url)\n",
    "\n",
    "for idx, img_url in enumerate(image_urls):\n",
    "    print(img_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
